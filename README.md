# LLM-Harmful-Output-Framework
This framework categorizes harmful outputs from large language models (LLMs).
